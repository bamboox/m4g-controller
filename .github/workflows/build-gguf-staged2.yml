# .github/workflows/build-gguf-staged.yml

name: Build GGUF (Staged)

on:
  workflow_dispatch:
    inputs:
      model_id:
        required: true
        default: 'Qwen/Qwen2.5-0.5B-Instruct'

jobs:
  build-and-convert:
    runs-on: ubuntu-latest
    
    steps:
      - name: Free disk space
        uses: jlumbroso/free-disk-space@main
        with:
          # this might remove tools that are actually needed,
          # if set to "true" but frees about 6GB
          tool-cache: true
          
          # all of these default to true, but feel free to set to
          # "false" if necessary for your workflow
          android: true
          dotnet: true
          haskell: true
          large-packages: true
          docker-images: true
          swap-storage: true
      
      - name: Checkout
        uses: actions/checkout@v4
      
      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'
      
      - name: Install dependencies
        run: |
          pip install huggingface_hub hf-transfer
      
      - name: Setup llama.cpp
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          # 下载最新 release 的 ubuntu-x64 预编译包 (排除 s390x, 但可能会包含 vulkan)
          gh release download -R ggerganov/llama.cpp -p "*bin-ubuntu-x64*"
          
          # 筛选出 CPU 版本 (不包含 vulkan 的文件)
          BINARY_FILE=$(ls *bin-ubuntu* | grep -v "vulkan" | head -n 1)
          
          if [ -z "$BINARY_FILE" ]; then
            echo "Error: No CPU binary found. Available files:"
            ls *bin-ubuntu-x64*
            exit 1
          fi
          
          echo "Using binary: $BINARY_FILE"
          
          # 解压
          if [[ "$BINARY_FILE" == *.zip ]]; then
            unzip "$BINARY_FILE" -d bin
          elif [[ "$BINARY_FILE" == *.tar.gz ]]; then
            mkdir -p bin
            tar -xzf "$BINARY_FILE" -C bin
          fi
          
          # Debug: 显示解压后的目录结构
          echo "Listing bin directory:"
          ls -R bin
          
          # 查找 llama-quantize 并移动到 bin/ 根目录
          QUANTIZE_PATH=$(find bin -name "llama-quantize" -type f | head -n 1)
          if [ -z "$QUANTIZE_PATH" ]; then
             # 尝试查找旧名称 quantize
             QUANTIZE_PATH=$(find bin -name "quantize" -type f | head -n 1)
          fi
          
          if [ -n "$QUANTIZE_PATH" ]; then
            echo "Found quantize binary at: $QUANTIZE_PATH"
            if [ "$QUANTIZE_PATH" != "bin/llama-quantize" ]; then
              mv "$QUANTIZE_PATH" bin/llama-quantize
            fi
          else
            echo "Error: llama-quantize binary not found!"
            exit 1
          fi
          
          # 赋予执行权限
          chmod +x bin/llama-quantize
          # 清理压缩包
          rm -f *bin-ubuntu-x64*
          
          git clone https://github.com/ggerganov/llama.cpp
          cd llama.cpp
          # 安装 python 依赖
          pip install -r requirements.txt
          cp convert_hf_to_gguf.py ../bin/
          cd ..
          rm -rf llama.cpp
      
      - name: Download model
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
          HF_HUB_ENABLE_HF_TRANSFER: 1
        run: |
          hf download ${{ github.event.inputs.model_id }} --local-dir ./model_hf --token "${HF_TOKEN}"
          # 清理 HF 缓存以释放空间
          rm -rf ~/.cache/huggingface
          echo "下载完成: $(du -sh ./model_hf)"
          df -h
      - name: Convert to GGUF
        run: |
          python bin/convert_hf_to_gguf.py ./model_hf \
            --outfile ./model.gguf \
            --outtype f16
          
          # 删除 HF 模型释放空间
          rm -rf ./model_hf
          
          echo "转换完成: $(du -sh ./model.gguf)"
      
      - name: Quantize
        run: |
          ./bin/llama-quantize ./model.gguf ./model-q4.gguf Q4_K_M
          
          # 删除 f16 释放空间
          rm ./model.gguf
          
          echo "量化完成: $(du -sh ./model-q4.gguf)"
      
      - name: Upload GGUF
        uses: actions/upload-artifact@v4
        with:
          name: model-gguf
          path: ./model-q4.gguf
