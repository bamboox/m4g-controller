# .github/workflows/build-gguf-staged.yml

name: Build GGUF (Staged)

on:
  workflow_dispatch:
    inputs:
      model_id:
        required: true
        default: 'zai-org/AutoGLM-Phone-9B'

jobs:
  build-and-convert:
    runs-on: ubuntu-latest
    
    steps:
      - name: Free disk space
        uses: jlumbroso/free-disk-space@main
        with:
          # this might remove tools that are actually needed,
          # if set to "true" but frees about 6GB
          tool-cache: true
          
          # all of these default to true, but feel free to set to
          # "false" if necessary for your workflow
          android: true
          dotnet: true
          haskell: true
          large-packages: true
          docker-images: true
          swap-storage: true
      
      - name: Checkout
        uses: actions/checkout@v4
      
      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'
      
      - name: Install dependencies
        run: |
          pip install huggingface_hub hf-transfer
      
      - name: Setup llama.cpp
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          # 下载最新 release 的 ubuntu-x64 预编译包 (排除 s390x, 但可能会包含 vulkan)
          gh release download -R ggerganov/llama.cpp -p "*bin-ubuntu-x64*"
          
          # 筛选出 CPU 版本 (不包含 vulkan 的文件)
          BINARY_FILE=$(ls *bin-ubuntu* | grep -v "vulkan" | head -n 1)
          
          if [ -z "$BINARY_FILE" ]; then
            echo "Error: No CPU binary found. Available files:"
            ls *bin-ubuntu-x64*
            exit 1
          fi
          
          echo "Using binary: $BINARY_FILE"
          
          # 创建 bin 目录
          mkdir -p bin
          
          # 解压
          if [[ "$BINARY_FILE" == *.zip ]]; then
            unzip -q "$BINARY_FILE" -d bin
          elif [[ "$BINARY_FILE" == *.tar.gz ]]; then
            tar -xzf "$BINARY_FILE" -C bin
          fi
          
          # 如果解压后 bin 下只有一个子目录，则将其内容移动到 bin 根目录
          # 例如: bin/llama-b1234-bin-ubuntu-x64/ -> bin/
          if [ $(ls -1 bin | wc -l) -eq 1 ]; then
            SUBDIR=$(ls -1 bin)
            if [ -d "bin/$SUBDIR" ]; then
              echo "Flattening directory structure from bin/$SUBDIR"
              mv "bin/$SUBDIR"/* bin/
              rmdir "bin/$SUBDIR"
            fi
          fi
          
          # Debug: 显示解压后的目录结构
          echo "Listing bin directory:"
          ls -R bin
          
          # 赋予执行权限
          chmod +x bin/*
          # 清理压缩包
          rm -f *bin-ubuntu-x64*
          
          git clone --depth 1 https://github.com/ggerganov/llama.cpp
          cd llama.cpp
          # 安装 python 依赖
          pip install -r requirements.txt
          #cp convert_hf_to_gguf.py ../bin/
          #cd ..
          #rm -rf llama.cpp
      
      - name: Download model
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
          HF_HUB_ENABLE_HF_TRANSFER: 1
        run: |
          hf download ${{ github.event.inputs.model_id }} --local-dir ./model_hf --token "${HF_TOKEN}"
          # 清理 HF 缓存以释放空间
          rm -rf ~/.cache/huggingface
          echo "下载完成: $(du -sh ./model_hf)"
          df -h
      - name: Convert to GGUF
        run: |
          cd llama.cpp
          python convert_hf_to_gguf.py ../model_hf \
            --outfile ../model.gguf \
            --outtype f16
          cd ..
          # 删除 HF 模型释放空间
          rm -rf ./model_hf
          rm -rf llama.cpp
          echo "转换完成: $(du -sh ./model.gguf)"
      
      - name: Quantize
        run: |
          ./bin/llama-quantize ./model.gguf ./model-q4.gguf Q4_K_M
          # 删除 f16 释放空间
          # rm ./model.gguf
          echo "量化完成: $(du -sh ./model-q4.gguf)"
      
      - name: Upload GGUF
        uses: actions/upload-artifact@v4
        with:
          name: model-gguf
          path: ./model-q4.gguf

      - name: Generate Release Tag
        id: tag
        run: |
          MODEL_NAME=$(echo "${{ github.event.inputs.model_id }}" | cut -d'/' -f2)
          DATE=$(date +'%Y%m%d-%H%M')
          TAG_NAME="${MODEL_NAME}-${DATE}"
          echo "tag_name=$TAG_NAME" >> $GITHUB_OUTPUT

      - name: Create Release
        uses: softprops/action-gh-release@v1
        with:
          tag_name: ${{ steps.tag.outputs.tag_name }}
          name: ${{ github.event.inputs.model_id }} (GGUF)
          body: |
            Auto-converted GGUF model.
            
            Source: ${{ github.event.inputs.model_id }}
            Quantization: Q4_K_M
          files: ./model-q4.gguf
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
